# -*- coding: utf-8 -*-
"""homework4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cngkvovnpNCoYdWSrJt6QJ5M8m_nAc-l
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.cm as cm

from sklearn.feature_extraction.text import TfidfVectorizer
newsgroups_train = fetch_20newsgroups(subset='train')
data_target = newsgroups_train.target
vectorizer = TfidfVectorizer(stop_words='english')
TFIDF = vectorizer.fit_transform(newsgroups_train.data)

data = TFIDF.toarray().T

url = "http://qwone.com/~jason/20Newsgroups/vocabulary.txt"
file = urllib.request.urlopen(url)

dict = {}
i=0
for line in file:
  dict[i] = line.decode("utf-8")
  i = i + 1

print("Data loaded for 20 news groups usign TF-IDF and removing english stop words. Also created the vocabulary dict")

print("PCA using orthogonal iteration methord for k=2")

f,d = data.shape
#k=2
theta = np.random.rand(f,2)
prevTheta = np.random.rand(f,2)

while((prevTheta != theta).all()):
    prevTheta = theta
    mat = data.T.dot(theta)
    mat = data.dot(mat)
    theta, eigen_vecs = np.linalg.qr(mat)

X_train_pca = data.T.dot(theta)

plt.figure(figsize=(10,10))
plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c=data_target,cmap=cm.rainbow)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.show()

print("PCA using orthogonal iteration methord for k=100")
k = 100
f,d = data.shape
theta = np.random.rand(f,k)
prevTheta = np.random.rand(f,k)

while((prevTheta != theta).all()):
    prevTheta = theta
    mat = data.T.dot(theta)
    mat = data.dot(mat)
    theta, eigen_vecs = np.linalg.qr(mat)

X_train_pca = data.T.dot(theta)

from sklearn.cluster import KMeans
from scipy.stats import multivariate_normal as mvn

class GMM:
    def __init__(self, C, n_runs):
        self.C = C
        self.n_runs = n_runs
  
    def get_params(self):
        return (self.mu, self.pi, self.sigma)

    def calculate_mean_covariance(self, X, prediction):
        d = X.shape[1]
        labels = np.unique(prediction)
        self.initial_means = np.zeros((self.C, d))
        self.initial_cov = np.zeros((self.C, d, d))
        self.initial_pi = np.zeros(self.C)
        
        counter=0
        for label in labels:
            ids = np.where(prediction == label)
            self.initial_pi[counter] = len(ids[0]) / X.shape[0]
            self.initial_means[counter,:] = np.mean(X[ids], axis = 0)
            de_meaned = X[ids] - self.initial_means[counter,:]
            Nk = X[ids].shape[0]
            self.initial_cov[counter,:, :] = np.dot(self.initial_pi[counter] * de_meaned.T, de_meaned) / Nk
            counter+=1
        assert np.sum(self.initial_pi) == 1    
            
        return (self.initial_means, self.initial_cov, self.initial_pi)

    def _initialise_parameters(self, X):
        n_clusters = self.C
        kmeans = KMeans(n_clusters= n_clusters, init="k-means++", max_iter=500, algorithm = 'auto')
        fitted = kmeans.fit(X)
        prediction = kmeans.predict(X)
        self._initial_means, self._initial_cov, self._initial_pi = self.calculate_mean_covariance(X, prediction)

        return (self._initial_means, self._initial_cov, self._initial_pi)

    def _e_step(self, X, pi, mu, sigma):
        N = X.shape[0] 
        self.gamma = np.zeros((N, self.C))

        const_c = np.zeros(self.C)

        self.mu = self.mu if self._initial_means is None else self._initial_means
        self.pi = self.pi if self._initial_pi is None else self._initial_pi
        self.sigma = self.sigma if self._initial_cov is None else self._initial_cov

        for c in range(self.C):
            self.gamma[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])

        gamma_norm = np.sum(self.gamma, axis=1)[:,np.newaxis]
        self.gamma /= gamma_norm

        return self.gamma

    def _m_step(self, X, gamma):
        N = X.shape[0]
        C = self.gamma.shape[1]
        d = X.shape[1]

        self.pi = np.mean(self.gamma, axis = 0)

        self.mu = np.dot(self.gamma.T, X) / np.sum(self.gamma, axis = 0)[:,np.newaxis]

        for c in range(C):
            x = X - self.mu[c, :]
            
            gamma_diag = np.diag(self.gamma[:,c])
            x_mu = np.matrix(x)
            gamma_diag = np.matrix(gamma_diag)

            sigma_c = x.T * gamma_diag * x
            self.sigma[c,:,:]=(sigma_c) / np.sum(self.gamma, axis = 0)[:,np.newaxis][c]

        return self.pi, self.mu, self.sigma
    
    
    def _compute_loss_function(self, X, pi, mu, sigma):
        N = X.shape[0]
        C = self.gamma.shape[1]
        self.loss = np.zeros((N, C))

        for c in range(C):
            dist = mvn(self.mu[c], self.sigma[c],allow_singular=True)
            self.loss[:,c] = self.gamma[:,c] * (np.log(self.pi[c]+0.00001)+dist.logpdf(X)-np.log(self.gamma[:,c]+0.000001))
        self.loss = np.sum(self.loss)
        return self.loss

    def fit(self, X):
        d = X.shape[1]
        self.mu, self.sigma, self.pi =  self._initialise_parameters(X)
        
        try:
            for run in range(self.n_runs):  
                self.gamma  = self._e_step(X, self.mu, self.pi, self.sigma)
                self.pi, self.mu, self.sigma = self._m_step(X, self.gamma)
                loss = self._compute_loss_function(X, self.pi, self.mu, self.sigma)
        
        except Exception as e:
            print(e)

        return self

    def predict(self, X):

        labels = np.zeros((X.shape[0], self.C))
        
        for c in range(self.C):
            labels [:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c],allow_singular=True)
        labels  = labels .argmax(1)
        return labels 

model = GMM(20, n_runs = 1000)

fitted_values = model.fit(X_train_pca)
predicted_values = model.predict(X_train_pca)

centers = np.zeros((20,100))
for i in range(model.C):
    density = mvn(cov=model.sigma[i], mean=model.mu[i],allow_singular=True).logpdf(X_train_pca)
    centers[i, :] = X_train_pca[np.argmax(density)]

print(X_train_pca.shape)
print(centers.shape)

theta_mean = np.matmul(X_train_pca,centers.transpose())

result = []
for i in range(20):
    temp = theta_mean[:,i]
    temp_index = temp.argsort()[-10:][::-1]
    print("Top 10 key words for cluster:",i)
    for j in temp_index:
      print(dict[j])
    
    
plt.figure(figsize = (10,8))
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1],c=predicted_values ,s=50, cmap=cm.rainbow, zorder=1)

plt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.5, zorder=2);

from sklearn.mixture import GaussianMixture
model = GaussianMixture(n_components=20, covariance_type='full')
output = model.fit(X_train_pca)
mean = model.means_


theta_mean = np.matmul(X_train_pca,mean.transpose())

result = []
for i in range(20):
    temp = theta_mean[:,i]
    temp_index = temp.argsort()[-10:][::-1]
    print("Top 10 key words for cluster:",i)
    for j in temp_index:
      print(dict[j])
    
    
plt.figure(figsize = (10,8))
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1],c=predicted_values ,s=50, cmap=cm.rainbow, zorder=1)

plt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.5, zorder=2);
